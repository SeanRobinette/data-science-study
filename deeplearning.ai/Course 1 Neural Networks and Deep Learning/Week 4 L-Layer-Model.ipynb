{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/L-Layer-Model.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initialization:**\n",
    "\n",
    "```initialize_parameters_deep(layer_dims)```, returns ```parameters```\n",
    "\n",
    "**Forward Propagation:**\n",
    "\n",
    "```L_model_forward(X, parameters)```, returns ```AL, caches```\n",
    "- ```linear_forward(A, W, b)```, returns ```Z, cache```\n",
    "- *```sigmoid(Z)```*, *```relu(Z)```*, returns ```A, activation_cache```\n",
    "- ```linear_activation_forward(A_prev, W, b, activation)```, returns ```A, cache```\n",
    "\n",
    "**Cost Function:**\n",
    "\n",
    "```compute_cost(AL, Y)```, returns ```cost```\n",
    "\n",
    "**Backward Propagation:**\n",
    "\n",
    "```L_model_backward(AL, Y, caches)```, returns ``` ```\n",
    "- ```linear_backward(dZ, cache)```, returns ```dA_prev, dW, db```\n",
    "- *```relu_backward(dA, activation_cache)```*, *```sigmoid_backward(dA, activation_cache)```*, returns ```dZ```\n",
    "- ```linear_activation_backward(dA, cache, activation)```, returns ```dA_prev, dW, db```\n",
    "\n",
    "**Update Parameters:**\n",
    "- ```update_parameters(parameters, grads, learning_rate)```, returns ``` ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "\n",
    "### 3.2: ```initialize_parameters_deep(layer_dims)```\n",
    "Initializes the parameters for the network.\n",
    "\n",
    "**Equations:**\n",
    "$$W^{[1]}, b^{[1]}, ..., W^{[L]}, b^{[L]}$$\n",
    "\n",
    "**Returns:**\n",
    "- ```parameters```, a dictionary with randomly initialized values for ```W1```, ```b1```, ... for ```L-1``` layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 ```linear_forward(A, W, b)```\n",
    "Performs the linear step of forward propagation for a single layer.\n",
    "\n",
    "**Equations:**\n",
    "$$Z = WA_{prev} + b$$\n",
    "\n",
    "**Returns:**\n",
    "- ```Z```, the linear activation\n",
    "- ```cache```, which just stores the given ```A```, ```W```, and ```b``` values for later use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 ```sigmoid(Z)```, ```relu(Z)```\n",
    "The sigmoid and ReLU activation functions, which are provided.\n",
    "\n",
    "**Equations:**\n",
    "$$A = g(Z)$$\n",
    "\n",
    "**Returns:**\n",
    "- ```A```, the non-linear activation value\n",
    "- ```activation_cache```, which just stores ```Z``` for later use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 ```linear_activation_forward(A_prev, W, b, activation)```\n",
    "Calls ```linear_forward(A,W,b)``` and ```sigmoid(Z)``` or ```relu(Z)``` to perform a complete forward propagation step.\n",
    "\n",
    "**Equations:**\n",
    "$$A = g(WA_{prev} + b)$$\n",
    "\n",
    "**Returns:**\n",
    "- ```A```, the activation value for the current layer\n",
    "- ```cache```, which stores A_prev, W, b, and Z for use in backward propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 ```L_model_forward(X, parameters)```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calls ```linear_activation_forward``` a total of ```L-1``` times using relu activation to compute the output of the hidden layers, then calls ```linear_activation_forward``` once more with a sigmoid activation to compute the final activation value of the network, completing an entire pass of forward propagation. Computes $A^{[L]}$\n",
    "\n",
    "**Returns:**\n",
    "- ```AL```, the value of $A^{[L]}$, the final vector of activation values for the network. This is the probability vector for this iteration.\n",
    "- ```caches```, which is just an array of all the caches from each layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 ```compute_cost(AL, Y)```\n",
    "\n",
    "Computes the cost function by averaging the binary cross-entropy loss function, $-[y\\log(\\hat{y})+(1-y)\\log(1-\\hat{y})]$, across all $m$ training examples.\n",
    "\n",
    "**Equations:**\n",
    "\n",
    "$$-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} (y^{(i)}\\log\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right))$$\n",
    "\n",
    "**Returns:**\n",
    "- ```cost```, the cost for the current iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 ```linear_backward(dZ, cache)```\n",
    "\n",
    "Performs the linear portion of backward propagation.\n",
    "\n",
    "**Equations:**\n",
    "\n",
    "```dW```:$\\large{\\frac{\\partial \\mathcal{J} }{\\partial W^{[l]}} = \\frac{1}{m} dZ^{[l]} A^{[l-1] T}}$\n",
    "- - -\n",
    "```db```:$\\large{\\frac{\\partial \\mathcal{J} }{\\partial b^{[l]}} = \\frac{1}{m} \\sum_{i = 1}^{m} dZ^{[l](i)}}$\n",
    "- - -\n",
    "```dA_prev```:$\\large{\\frac{\\partial \\mathcal{L} }{\\partial A^{[l-1]}} = W^{[l] T} dZ^{[l]}}$\\\n",
    "**Returns:**\n",
    "- ```dW```, ```db```, and ```dA_prev```, the gradients for the current layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 ```relu_backward(dA, activation_cache```, ```sigmoid_backward(dA, activation_cache)```\n",
    "Performs the backward propagation step for the ReLU and sigmoid activation functions. Provided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 ```linear_activation_backward(dA, cache, activation)```\n",
    "Calls ```linear_backward``` and either ```relu_backward``` or ```sigmoid_backward``` to perform a full backward propagation step for a single layer.\n",
    "\n",
    "**Returns:**\n",
    "- ```dW```, ```db```, and ```dA_prev```, the gradients for the current layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 ```L_model_backward(AL, Y, caches)```\n",
    "Calls ```linear_activation_backward``` a total of ```L``` times to compute the gradients for the current iteration, completing an entire backward propagation pass.\n",
    "\n",
    "**Returns:**\n",
    "- ```grads```, a dictionary containing the gradients ```dA1```, ```dW1```, ```db1```, ..., ```dAL```, ```dWL```, ```dbL```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 ```update_parameters(parameters, grads, learning_rate)```\n",
    "**Equations:**\n",
    "$$ W^{[l]} = W^{[l]} - \\alpha dW^{[l]}$$\n",
    "$$ b^{[l]} = b^{[l]} - \\alpha db^{[l]}$$\n",
    "**Returns:**\n",
    "- ```parameters```, a dictionary containing the updated parameters after one full iteration, ```W1```, ```b1```, ..., ```WL```, ```BL```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
